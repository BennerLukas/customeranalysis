{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% md\n",
    "\n",
    "# Customer Analysis - Explore Customer Behavior\n",
    "\n",
    "#%% md\n",
    "\n",
    "## Import\n",
    "\n",
    "#%% md\n",
    "\n",
    "Needed packages. Pyspark will be used for data managing and plotly for visualisations. Keep in mind to install\n",
    "JAVA so Spark will work properly.\n",
    "\n",
    "Used dataset is from https://rees46.com/de found on https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store.\n",
    "\n",
    "#%%\n",
    "\n",
    "%timeit\n",
    "import os\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#%% md\n",
    "\n",
    "## Read\n",
    "\n",
    "#%% md\n",
    "\n",
    "The data needs to be located in ```data/``` in unzipped form as a csv.\n",
    "\n",
    "#%%\n",
    "\n",
    "# spark = pyspark.sql.SparkSession.builder.appName(\"app1\").getOrCreate()\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"app_great\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.executor.memory\", f\"16g\") \\\n",
    "    .config(\"spark.driver.memory\", f\"16g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", f\"16g\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", f\"16\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", f\"4g\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#%%\n",
    "\n",
    "# read raw data\n",
    "sdf_201911 = spark.read.csv(\"data/2019-Nov.csv\", header=True, inferSchema=True)\n",
    "sdf_201910 = spark.read.csv(\"data/2019-Oct.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# join both months together\n",
    "sdf = sdf_201910.union(sdf_201911)\n",
    "\n",
    "#%%\n",
    "\n",
    "# sdf = spark.read.csv(\"data/test_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf.show(3)\n",
    "\n",
    "#%% md\n",
    "\n",
    "## Preparation\n",
    "\n",
    "#%% md\n",
    "\n",
    "Prepare and enhance data for analysis and modelling.\n",
    "\n",
    "#%%\n",
    "\n",
    "# Datatypes\n",
    "sdf = sdf.withColumn(\"event_time\", sdf[\"event_time\"].cast(pyspark.sql.types.TimestampType()))\n",
    "sdf = sdf.withColumn(\"category_id\", sdf[\"category_id\"].cast(pyspark.sql.types.StringType()))\n",
    "sdf = sdf.withColumn(\"product_id\", sdf[\"product_id\"].cast(pyspark.sql.types.StringType()))\n",
    "sdf = sdf.withColumn(\"user_id\", sdf[\"user_id\"].cast(pyspark.sql.types.StringType()))\n",
    "\n",
    "#%%\n",
    "\n",
    "# Feature Splitting\n",
    "# sdf = sdf.withColumn(\"category_class\", f.substring_index(sdf.category_code, '.', 1))\n",
    "\n",
    "sdf = sdf.withColumn(\"category_class\", f.split(sdf[\"category_code\"], \"\\.\").getItem(0))\n",
    "sdf = sdf.withColumn(\"category_sub_class\", f.split(sdf[\"category_code\"], \"\\.\").getItem(1))\n",
    "sdf = sdf.withColumn(\"category_sub_sub_class\", f.split(sdf[\"category_code\"], \"\\.\").getItem(2))\n",
    "\n",
    "sdf = sdf.withColumn(\"year\", f.year(\"event_time\"))\n",
    "sdf = sdf.withColumn(\"month\", f.month(\"event_time\"))\n",
    "sdf = sdf.withColumn(\"weekofyear\", f.weekofyear(\"event_time\"))\n",
    "sdf = sdf.withColumn(\"dayofyear\", f.dayofyear(\"event_time\"))\n",
    "sdf = sdf.withColumn(\"dayofweek\", f.dayofweek(\"event_time\"))\n",
    "sdf = sdf.withColumn(\"dayofmonth\", f.dayofmonth(\"event_time\"))\n",
    "sdf = sdf.withColumn(\"hour\", f.hour(\"event_time\"))\n",
    "\n",
    "sdf = sdf.withColumn('turnover', f.when(f.col('event_type') == 'purchase', f.col('price')).otherwise(0))\n",
    "sdf = sdf.withColumn('bougth_quantity', f.when(f.col('event_type') == 'purchase', f.lit(1)).otherwise(0))\n",
    "sdf = sdf.withColumn('viewed_quantity', f.when(f.col('event_type') == 'view', f.lit(1)).otherwise(0))\n",
    "sdf = sdf.withColumn('cart_quantity', f.when(f.col('event_type') == 'cart', f.lit(1)).otherwise(0))\n",
    "# None Handling\n",
    "# sdf = sdf.fillna(value=\"not defined\")\n",
    "\n",
    "sdf.printSchema()\n",
    "\n",
    "#%% md\n",
    "\n",
    "## Dataframe Creation\n",
    "\n",
    "#%% md\n",
    "\n",
    "create several dataframes with different aggregation level to answer different questions/ tasks.\n",
    "\n",
    "#%%\n",
    "\n",
    "# raw\n",
    "sdf_raw = sdf\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf.createOrReplaceTempView(\"Data\")\n",
    "\n",
    "#%% md\n",
    "\n",
    "## Field Explanations\n",
    "\n",
    "#%% md\n",
    "\n",
    "Following fields are in the standard dataset:\n",
    "- event_time\n",
    "- product_id\n",
    "- category_id\n",
    "- category_code\n",
    "- brand\n",
    "- price\n",
    "- user_id\n",
    "- user_session\n",
    "\n",
    "#%% md\n",
    "\n",
    "### General\n",
    "In this overview you can see the count of unique rows, product_ids, category_classes, category_codes, category_ids, brands, user_ids and user_sessions as well as the average price of the products.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_count_overview = spark.sql(\"SELECT COUNT(*) AS Row_Count, \\\n",
    "                                       COUNT(DISTINCT(product_id)) AS Product_ID, \\\n",
    "                                       COUNT(DISTINCT(category_class)) AS Category_Class, \\\n",
    "                                       COUNT(DISTINCT(category_code)) AS Category_Code, \\\n",
    "                                       COUNT(DISTINCT(category_id)) AS Category_ID, \\\n",
    "                                       COUNT(DISTINCT(brand)) AS Brand, \\\n",
    "                                       COUNT(DISTINCT(user_id)) AS User_ID, \\\n",
    "                                       COUNT(DISTINCT(user_session)) AS User_Session, \\\n",
    "                                       ROUND(MEAN(price),2) AS AVG_Price \\\n",
    "                                FROM Data\")\n",
    "sdf_count_overview.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_raw.select(\"product_id\", \"category_class\", \"category_code\", \"category_id\", \"brand\", \"user_id\", \"user_session\", \"price\").describe().show()\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_raw.show(1, vertical=True)\n",
    "\n",
    "\n",
    "#%% md\n",
    "\n",
    "### event_time\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_raw.select(\"event_time\").show(5)\n",
    "\n",
    "\n",
    "#%% md\n",
    "\n",
    "### event_type\n",
    "The event_type describes the kind of interaction, an user had with a product. The field can take three forms: View, Cart and Purchase. The distribution of these three forms is represented in the following plot:\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_event_type_dist = sdf_raw.groupBy(\"event_type\").count()\n",
    "sdf_event_type_dist.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "# Plot Event Types\n",
    "df = sdf_event_type_dist.select(\"count\", \"event_type\").toPandas()\n",
    "fig = px.pie(df, values='count', names='event_type', title='Distribution of Customer Actions')\n",
    "fig.show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "### product_id\n",
    "The product_id is the unique identificator for a product. As you can see in the overview, there are ... unique product_ids in the datasets Oct-2019 and Nov-2019, the users have interacted with.\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "sdf_count_per_product_id = sdf_raw.groupBy(\"product_id\").count().orderBy(f.desc(\"count\"))\n",
    "\n",
    "px.bar(sdf_count_per_product_id.limit(10).toPandas(), x='product_id', y='count', title=\"Top 10 most interacted products\")\n",
    "\n",
    "\n",
    "#%% md\n",
    "\n",
    "### category_id\n",
    "The category_id is an unique identifier for the category of a Product. Every Product is assigned to a single category_id, which is summarizing many product_ids into categories. This knowledge is based on the more detailed analyzes within the file \"product_analysis.ipnynb\". As you can see in the overview, there are ... unique category_ids.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_count_per_category_id = sdf_raw.groupBy(\"category_id\").count().orderBy(f.desc(\"count\"))\n",
    "\n",
    "\n",
    "px.bar(sdf_count_per_category_id.limit(10).toPandas(), x=\"category_id\", y=\"count\", title=\"Top 10  category_ids most interacted with\")  \n",
    "\n",
    "#%% md\n",
    "\n",
    "### category_code\n",
    "The category_code is describing the category, a product_id and category_id is assigned to. Every Product_id and Category_id is assigned to a single category_code, which is summarizing many product_ids and category_ids into categories. This knowledge is also based on the more detailed analyzes within the file \"product_analysis.ipnynb\". As you can see in the overview, there are ... unique category_code.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_count_per_category_code = sdf_raw.groupBy(\"category_code\").count().orderBy(f.desc(\"count\"))\n",
    "\n",
    "                                        \n",
    "px.bar(sdf_count_per_category_code.limit(10).toPandas(), x=\"category_code\", y=\"count\", title=\"Top 10 category_code most interacted with\")\n",
    "\n",
    "#%% md\n",
    "\n",
    "\n",
    "\n",
    "#%% md\n",
    "\n",
    "### brand\n",
    "The brand indicates the brand of a product_id. It is independent of the categories, so that a brand can appear in many category_classes. This knowledge is also based on the more detailed analyzes within the file \"product_analysis.ipnynb\". There are ... unique brands in the dataset. Thereby you can see the most popular brands in the following plot:\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_count_per_brand = sdf_raw.groupBy(\"brand\").count().orderBy(f.desc(\"count\"))\n",
    "\n",
    "px.histogram(sdf_count_per_brand.limit(10).toPandas(), x=\"brand\", y=\"count\", title=\"Top 10 brands most interacted with\")                               \n",
    "\n",
    "#%% md\n",
    "\n",
    "### price\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_raw.describe(\"price\").show()\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_avg_prices = sdf_raw.select(\"product_id\", \"price\").distinct()\n",
    "px.box(sdf_avg_prices.toPandas(), y=\"price\", title=\"Prices for every product\")\n",
    "\n",
    "\n",
    "#%% md\n",
    "\n",
    "### user_id\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_raw.select(\"user_id\").show(5)\n",
    "\n",
    "#%%\n",
    "\n",
    "# creating spark dataframe for unique number of users each month\n",
    "\n",
    "sdf_user_id_by_month = sdf_raw.select(\"user_id\", \"month\").distinct().groupBy(\"month\").count()\n",
    "\n",
    "#%%\n",
    "\n",
    "# Data Prep for user graph\n",
    "\n",
    "pdf_usr_id_by_mnth = sdf_user_id_by_month.toPandas()\n",
    "pdf_usr_id_by_mnth = pdf_usr_id_by_mnth[pdf_usr_id_by_mnth.month != 12]\n",
    "pdf_usr_id_by_mnth.month = pdf_usr_id_by_mnth.month.astype(str)\n",
    "px.bar(pdf_usr_id_by_mnth, x=\"month\", y=\"count\", title=\"Unique users each month\")\n",
    "\n",
    "#%% md\n",
    "\n",
    "In October 3.02 million people visited the site and in November the total user count grew to 3.696 million. Which nets\n",
    "a difference of around 700,000 users.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_user_id_both_months = sdf_raw.select(\"user_id\", \"month\").distinct().groupby(\"user_id\").sum()\n",
    "\n",
    "cnt = sdf_user_id_both_months.where(sdf_user_id_both_months[\"sum(month)\"] == 21).count()\n",
    "print(\"Amount of users that visited the site in both months:\", cnt)\n",
    "\n",
    "#%% md\n",
    "\n",
    "Not all users in October and November are to be expected to have only visited in the respective month.\n",
    "A total of 1,400,979 users visited the page in both October and November.\n",
    "\n",
    "#%% md\n",
    "\n",
    "### user_session\n",
    "\n",
    "#%%\n",
    "\n",
    "# avg actions per session\n",
    "sdf_cnt_action_per_session = sdf_raw.groupby(\"user_session\").count()\n",
    "sdf_cnt_action_per_session.describe().show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "## Feature Enginnering\n",
    "\n",
    "#%% md\n",
    "\n",
    "### category_class and category_sub_class\n",
    "The category_code consists of two or three parts in general, which are separeted by a dot. A possible category_code is for example: appliances.kitchen.washer or electronics.smartphone. Because of that the category code can be splited into to the categories: category_class, category_sub_class and category_sub_sub_class.\n",
    "\n",
    "The category_class is representing the fist part of the category_code. It can be used to summarize many category_codes into an overarching category_class. As you can see in the overview, there are ... unique category_classes.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_agg_classes = sdf_raw.groupBy(\"category_class\", \"category_sub_class\", \"category_sub_sub_class\").count().na.fill(value=\"not defined\")\n",
    "sdf_agg_classes = sdf_agg_classes.where(sdf_agg_classes[\"category_class\"] != \"not defined\")\n",
    "sdf_agg_classes.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "px.sunburst(sdf_agg_classes.toPandas(), path=[\"category_class\", \"category_sub_class\", \"category_sub_sub_class\"], values=\"count\", title=\"Category Classes and Subclasses (without not defined data in category_class)\")\n",
    "\n",
    "#%% md\n",
    "\n",
    "### Time - Fields\n",
    "\n",
    "#%% md\n",
    "\n",
    "The column “event_time” allows you to create the following columns: year, month, weekofyear, dayofyear, dayofweek, dayofmonth and hour. These columns allow advanced analysis.\n",
    "\n",
    "- Year: The Year-column contains only the year 2019 since the dataset only covers this year.\n",
    "- Month: The month-column cotains the values 10, 11 and 12,  which are representing the october, november and december 2019.DeprecationWarning\n",
    "- Weekofyear: The weekofyear-column is covering the weeks ... - ...\n",
    "- Dayofyear: The dayofyear-column is covering the days .... - ...\n",
    "- Dayofweek: The dayofweek-column is covering the values 1-7. These values are representing days sunday(1), monday(2), Tuesday(3), Wednesday(4), Thursday(5), Friday(6) and Saturday(7).\n",
    "- Dayofmonth: The dayofmonth-column contains the values 1-31, which are reperesenting the day in the corresponding month.\n",
    "- hour: The hour-column contains the values 0-24, which are representing the hour of interaction.\n",
    "\n",
    "#%% md\n",
    "\n",
    "### Turnover and quantities\n",
    "\n",
    "#%% md\n",
    "\n",
    "In the following table you can see that in some cases a product ID was purchased several times per user session.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_count_session_product = sdf_raw.where(sdf_raw[\"event_Type\"] == 'purchase').groupBy(\"user_session\", \"product_id\", \"event_type\").count()\n",
    "\n",
    "sdf_count_session_product.show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "From this it can be concluded that a single interaction is created for each product purchased. With the help of this information, the columns \"turnover\", \"bought_quantity\", \"viewed_quantity\", \"cart_quantity\" can be created.\n",
    "\n",
    "- Turnover: The turnover is equivalent to the price, if the event_type is equal to \"purchase\".\n",
    "- bought_quantity: The bought_quantity describes the quantity of a product,that had been bought. In the unaggregated form it only contains the values 0 and 1.\n",
    "- viewed_quantity: The viewed_quantity describes the quantity of a product,that had been viewed. In the unaggregated form it only contains the values 0 and 1.\n",
    "- cart_quantity: The cart_quantity describes the quantity of a product,that had been put into the cart. In the unaggregated form it only contains the values 0 and 1.\n",
    "\n",
    "These columns are particularly suitable for aggregated analyses.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_raw.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_count_turnover_quantity = sdf_raw.agg(f.sum(\"turnover\"), f.sum(\"bougth_quantity\"), f.sum(\"viewed_quantity\"), f.sum(\"Cart_quantity\"))                                       \n",
    "sdf_count_turnover_quantity.show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "## Exploration and Analysis\n",
    "\n",
    "#%% md\n",
    "\n",
    "### Time Distribution\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Time and Events\n",
    "\n",
    "#%%\n",
    "\n",
    "# aggregated time (weeks, dayofweeks, month)\n",
    "\n",
    "sdf_time_dist_month = sdf.groupBy(\"event_type\", \"dayofmonth\").count()\n",
    "sdf_time_dist_month = sdf_time_dist_month.withColumnRenamed(\"count\", \"cnt\")\n",
    "sdf_time_dist_month = sdf_time_dist_month.sort(\"dayofmonth\", \"event_type\")\n",
    "sdf_time_dist_week = sdf.groupBy(\"event_type\", \"dayofweek\").count()\n",
    "sdf_time_dist_week = sdf_time_dist_week.withColumnRenamed(\"count\", \"cnt\")\n",
    "sdf_time_dist_week = sdf_time_dist_week.sort(\"dayofweek\", \"event_type\")\n",
    "\n",
    "sdf_time_dist_day = sdf.groupBy(\"event_type\", \"hour\").count()\n",
    "sdf_time_dist_day = sdf_time_dist_day.withColumnRenamed(\"count\", \"cnt\")\n",
    "sdf_time_dist_day = sdf_time_dist_day.sort(\"hour\", \"event_type\")\n",
    "\n",
    "#%%\n",
    "\n",
    "# Timestamp Distribution (per event_type) over every day of month\n",
    "\n",
    "df = sdf_time_dist_month.toPandas()\n",
    "\n",
    "fig = px.bar(df, x = 'dayofmonth', y = 'cnt', color ='event_type', barmode = 'stack')\n",
    "\n",
    "fig.update_layout(title = \"Number of events over a month\",\n",
    "     xaxis_title = 'Day of Month', yaxis_title = 'Number of Events')\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "# Timestamp Distribution (per event_type) over every day of week\n",
    "\n",
    "df = sdf_time_dist_week.toPandas()\n",
    "\n",
    "fig = px.bar(df, x = 'dayofweek', y = 'cnt', color ='event_type', barmode = 'stack')\n",
    "\n",
    "fig.update_layout(title = \"Number of events over a week\",\n",
    "     xaxis_title = 'Day of Week', yaxis_title = 'Number of Events')\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "# Timestamp Distribution (per event_type) over every hour of a day\n",
    "\n",
    "df = sdf_time_dist_day.toPandas()\n",
    "\n",
    "fig = px.bar(df, x = 'hour', y = 'cnt', color ='event_type', barmode = 'stack')\n",
    "\n",
    "fig.update_layout(title = \"Number of events over a day\",\n",
    "     xaxis_title = 'Hour of day', yaxis_title = 'Number of Events')\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Time and Turnover\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "sdf_month_Umsatz = sdf_raw.groupBy(\"month\", \"dayofmonth\").sum(\"turnover\").orderBy(f.asc(\"month\"), f.asc(\"dayofmonth\"))                            \n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_month_Umsatz = sdf_month_Umsatz.withColumn(\"month\", sdf_month_Umsatz[\"month\"].cast(pyspark.sql.types.StringType()))\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "df = sdf_month_Umsatz.toPandas()\n",
    "\n",
    "fig = px.bar(df, x = \"dayofmonth\", y = \"sum(turnover)\", color ='month', barmode = 'stack')\n",
    "\n",
    "fig.update_layout(title = \"Turnover per Month\",\n",
    "     xaxis_title = 'Day of Month', yaxis_title = 'Turnover')\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_week_Umsatz = sdf_raw.groupBy(\"weekofyear\", \"dayofweek\").sum(\"turnover\").orderBy(f.asc(\"weekofyear\"), f.asc(\"dayofweek\"))                            \n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_week_Umsatz = sdf_week_Umsatz.withColumn(\"weekofyear\", sdf_week_Umsatz[\"weekofyear\"].cast(pyspark.sql.types.StringType()))\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "df = sdf_week_Umsatz.toPandas()\n",
    "\n",
    "fig = px.bar(df, x = \"dayofweek\", y = \"sum(turnover)\", color ='weekofyear', barmode = 'stack')\n",
    "\n",
    "fig.update_layout(title = \"Turnover per Month\",\n",
    "     xaxis_title = 'Day of Week', yaxis_title = 'Turnover')\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "### Category and products\n",
    "\n",
    "#### Connection between category_class, category_code, category_id, product_id and brand\n",
    "\n",
    "Connection between category_class, category_code, category_id, product_id and brand\n",
    "The product_id is a subset of the category_id, which is a subset of the category_code. The category_code is in turn a subset of the category_class. (product_id ⊂ category_id ⊂ category_code ⊂ category_class). The brand on the otherhand is cross-class. This knowledge is based on the more detailed analyzes within the file \"product_analysis.ipnynb\".\n",
    "\n",
    "In the following plot you can see distribution of the product_id, category_id and category_code within the category_class. It´s possible to access a more detailed view by selecting a special category_class, category_code or category_id.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_product_per_category = sdf_raw.groupBy(\"category_id\").agg(f.countDistinct(\"product_id\"))\n",
    "\n",
    "df = sdf_product_per_category.toPandas()\n",
    "px.box(df, y=\"count(product_id)\", title=\"Number of products per category_id\")\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_agg_brand_category = sdf_raw.groupBy(\"category_class\", \"brand\", \"product_id\").count().na.drop()\n",
    "px.sunburst(sdf_agg_brand_category.toPandas(), path=[\"category_class\", \"brand\"], values=\"count\", title=\"Brands per Category_class (without not defined data\")\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Connection to the price\n",
    "\n",
    "The following plots will represent the price distribution within the category_classes, category_codes, category_ids, product_ids and brands.\n",
    "\n",
    "#%%\n",
    "\n",
    "px.box(sdf_raw.select(\"category_class\", \"price\").toPandas(), x=\"category_class\", y=\"price\", title=\"Price ~ Category_class\")\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_price_per_product = sdf_raw.select(\"product_id\", \"price\").distinct().orderBy(f.desc(\"price\"))\n",
    "\n",
    "sdf_price_per_product.show(10)\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Connection to the event-type\n",
    "The following plots will represent the event_type distribution within the category_classes, category_codes, category_ids, product_ids and brands.\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_category_class_event_distribution = sdf_raw.groupBy(\"category_class\", \"event_type\").count().na.drop()   \n",
    "\n",
    "px.sunburst(sdf_category_class_event_distribution.toPandas(), path=['category_class','event_type'], values=\"count\", title=\"Event Type per Category_Class (no None Values)\")\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Connection to the turnover\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_brand_overview = sdf_raw.groupBy(\"brand\").agg(f.sum(\"turnover\"), f.avg(\"price\"), f.sum(\"viewed_quantity\"), f.sum(\"cart_quantity\"), f.sum(\"bougth_quantity\")).orderBy(f.desc(\"sum(turnover)\"))\n",
    "sdf_brand_overview = sdf_brand_overview.withColumn(\"sum(turnover)\", f.round(sdf_brand_overview[\"sum(turnover)\"], 2))\n",
    "sdf_brand_overview = sdf_brand_overview.withColumn(\"avg(price)\", f.round(sdf_brand_overview[\"avg(price)\"], 2))\n",
    "\n",
    "sdf_brand_overview.show(10)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_category_code_overview = sdf_raw.groupBy(\"category_code\").agg(f.sum(\"turnover\"), f.avg(\"price\"), f.sum(\"viewed_quantity\"), f.sum(\"cart_quantity\"), f.sum(\"bougth_quantity\")).orderBy(f.desc(\"sum(turnover)\"))\n",
    "sdf_category_code_overview = sdf_category_code_overview.withColumn(\"sum(turnover)\", f.round(sdf_category_code_overview[\"sum(turnover)\"], 2))\n",
    "sdf_category_code_overview = sdf_category_code_overview.withColumn(\"avg(price)\", f.round(sdf_category_code_overview[\"avg(price)\"], 2))\n",
    "\n",
    "sdf_category_code_overview.show(10)\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    " sdf_category_class_overview = sdf_raw.groupBy(\"category_class\").agg(f.sum(\"turnover\"), f.avg(\"price\"), f.sum(\"viewed_quantity\"), f.sum(\"cart_quantity\"), f.sum(\"bougth_quantity\")).orderBy(f.desc(\"sum(turnover)\"))\n",
    "sdf_category_class_overview = sdf_category_class_overview.withColumn(\"sum(turnover)\", f.round(sdf_category_class_overview[\"sum(turnover)\"], 2))\n",
    "sdf_category_class_overview = sdf_category_class_overview.withColumn(\"avg(price)\", f.round(sdf_category_class_overview[\"avg(price)\"], 2))\n",
    "\n",
    "sdf_category_class_overview.show(10)\n",
    "\n",
    "\n",
    "#%% md\n",
    "\n",
    "### Event_Type and Price\n",
    "The following plot represents the distribution of the price within the event_type.\n",
    "\n",
    "#%% md\n",
    "\n",
    "### User analysis\n",
    "\n",
    "Overview:\n",
    "- How many sessions does a user have on average?\n",
    "- How many products does a user buy / view / put in cart on average?\n",
    "- How many interactions does a user have within one session on average?\n",
    "- Avrg Sessions per (Week)Day => Boxplot?\n",
    "\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### How many sessions does a user have on average?\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_agg_user = sdf_raw.select(\"user_id\", \"user_session\").distinct().groupBy(\"user_id\").count()\n",
    "sdf_agg_user.select(\"count\").describe().show()\n",
    "\n",
    "#%%\n",
    "\n",
    "print(\"Statistical distribution of Sessions per User in October\")\n",
    "sdf_agg_user_mnth = sdf_raw.select(\"user_id\", \"user_session\", \"month\").distinct().groupBy(\"user_id\", \"month\").count()\n",
    "sdf_agg_user_mnth.where(sdf_agg_user_mnth.month == 10).select(\"count\").describe().show()\n",
    "\n",
    "print(\"Statistical distribution of Sessions per User in November\")\n",
    "sdf_agg_user_mnth = sdf_raw.select(\"user_id\", \"user_session\", \"month\").distinct().groupBy(\"user_id\", \"month\").count()\n",
    "sdf_agg_user_mnth.where(sdf_agg_user_mnth.month == 11).select(\"count\").describe().show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### How many products does a user buy / view / put in cart on average?\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_amount_events = sdf_raw.select(\"user_id\", \"event_type\").groupBy(\"event_type\").count()\n",
    "amount_usr = sdf_raw.select(\"user_id\").distinct().count()\n",
    "pdf_avrg_events_usr = sdf_amount_events.toPandas()\n",
    "\n",
    "pdf_avrg_events_usr[\"count\"] = pdf_avrg_events_usr[\"count\"].div(amount_usr)\n",
    "print(pdf_avrg_events_usr)\n",
    "\n",
    "#%% md\n",
    "\n",
    "A user views on average XX item, puts XX items in their cart and buys XX items.\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_agg_user_type = sdf_raw.select(\"user_id\", \"event_type\").groupBy(\"user_id\").count()\n",
    "most_active_user = sdf_agg_user_type.sort(\"count\", ascending=False).take(1)\n",
    "\n",
    "print(f\"The most active user has the ID {most_active_user[0][0]} with a total of {most_active_user[0][1]} interactions.\")\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### How many interactions does a user have within one session on average? - done\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_amount_interactions_per_sess = sdf_raw.select(\"user_session\", \"event_type\").groupBy(\"user_session\", \"event_type\").count()\n",
    "# sdf_amount_interactions_per_sess.show()\n",
    "print(\"Average amount of views per session:\")\n",
    "sdf_amount_interactions_per_sess.where(sdf_amount_interactions_per_sess.event_type == \"view\").agg({\"count\":\"mean\"}).show()\n",
    "print(\"Average amount of 'Add to Cart' per session:\")\n",
    "sdf_amount_interactions_per_sess.where(sdf_amount_interactions_per_sess.event_type == \"cart\").agg({\"count\":\"mean\"}).show()\n",
    "print(\"Average amount of purchases per session:\")\n",
    "sdf_amount_interactions_per_sess.where(sdf_amount_interactions_per_sess.event_type == \"purchase\").agg({\"count\":\"mean\"}).show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Avrg Sessions per week day\n",
    "\n",
    "#%%\n",
    "\n",
    "# User session distribution over every day of week\n",
    "\n",
    "sdf_usr_ses_dist_week = sdf_raw.select(\"user_session\", \"dayofyear\", \"dayofweek\").groupBy(\"user_session\",\"dayofyear\",\"dayofweek\").count()\n",
    "pdf_usr_sess_time_dist = sdf_usr_ses_dist_week.toPandas()\n",
    "fig_usr_time = px.box(pdf_usr_sess_time_dist, x=\"dayofweek\", y=\"Session\", title=\"Average Sessions per Weekday\")\n",
    "fig_usr_time.update_layout(\n",
    "    xaxis_title=\"Day of Week\"\n",
    ")\n",
    "\n",
    "#%% md\n",
    "\n",
    "### Unique sessions each month\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_ses_by_mnth = sdf_raw.select(\"user_session\", \"month\").distinct().groupby(\"month\").count()\n",
    "\n",
    "pdf_usr_ses_by_mnth = sdf_ses_by_mnth.toPandas()\n",
    "pdf_usr_ses_by_mnth = pdf_usr_ses_by_mnth[pdf_usr_ses_by_mnth.month != 12]\n",
    "pdf_usr_ses_by_mnth.month = pdf_usr_id_by_mnth.month.astype(str)\n",
    "px.bar(pdf_usr_id_by_mnth, x=\"month\", y=\"count\", title=\"Unique sessions each month\")\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### User and turnover\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_user_overview = sdf_raw.groupBy(\"user_id\").agg(f.sum(\"turnover\"), f.count(\"user_session\"), f.sum(\"viewed_quantity\"), f.sum(\"cart_quantity\"), f.sum(\"bougth_quantity\")).orderBy(f.desc(\"sum(turnover)\"))                                \n",
    "sdf_user_overview.show(10)\n",
    "\n",
    "#%% md\n",
    "\n",
    "### session analysis\n",
    "\n",
    "\n",
    "\n",
    "#%% md\n",
    "\n",
    "- number of events\n",
    "- session_start_time\n",
    "- session_stop_time\n",
    "- session_success\n",
    "- products_bought\n",
    "- products_viewed\n",
    "- turnover\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_session = sdf.select(\"user_id\", \"user_session\", \"event_type\", \"product_id\", \"price\", \"event_time\") #.orderBy(\"user_id\", \"user_session\")\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_session = sdf_session.withColumn(\"views\", f.when(sdf_session.event_type == \"view\", 1).otherwise(0))\n",
    "sdf_session = sdf_session.withColumn(\"purchases\", f.when(sdf_session.event_type == \"purchase\", 1).otherwise(0))\n",
    "sdf_session = sdf_session.withColumn(\"carts\", f.when(sdf_session.event_type == \"cart\", 1).otherwise(0))\n",
    "sdf_session = sdf_session.withColumn(\"turnover\", f.when(sdf_session.event_type == \"purchase\", sdf_session[\"price\"]).otherwise(0))\n",
    "\n",
    "sdf_session = sdf_session.withColumn(\"first_event\", sdf_session.event_time)\n",
    "sdf_session = sdf_session.withColumn(\"last_event\", sdf_session.event_time)\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_session_agg = sdf_session.groupBy(\"user_id\", \"user_session\").agg(f.sum(\"turnover\"), f.sum(\"views\"), f.sum(\"purchases\"), f.sum(\"carts\"), f.min(\"event_time\"), f.max(\"event_time\"))\n",
    "sdf_session_agg = sdf_session_agg.withColumn(\"duration\", (sdf_session_agg[\"max(event_time)\"] - sdf_session_agg[\"min(event_time)\"]))\n",
    "sdf_session_agg = sdf_session_agg.withColumn(\"sum(events)\", (sdf_session_agg[\"sum(views)\"] + sdf_session_agg[\"sum(purchases)\"] + sdf_session_agg[\"sum(carts)\"]))\n",
    "sdf_session_agg = sdf_session_agg.withColumn(\"turnover\", f.when(sdf_session_agg[\"sum(purchases)\"] > 0, (sdf_session_agg[\"sum(purchases)\"] *  sdf_session_agg[\"sum(turnover)\"])).otherwise(0))\n",
    "\n",
    "sdf_session_agg = sdf_session_agg.withColumn(\"successfull\", f.when(sdf_session_agg[\"sum(purchases)\"] > 0, 1).otherwise(0))\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_session_agg.printSchema()\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_session_agg.show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "### Customer Profiles\n",
    "\n",
    "#%% md\n",
    "\n",
    "In preparation for clustering a customer profile will be created:\n",
    "\n",
    "- customer_id\n",
    "- number_of_view_events\n",
    "- number_of_cart_events\n",
    "- number_of_purchase_events\n",
    "- total_turnover\n",
    "- number_of_bought_items (resolve multiple purchasing events for quantity)\n",
    "- avg_sold_cart\n",
    "- avg_session_time\n",
    "- avg_actions_per_session\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# time period simply dependend on input data\n",
    "sdf_customer_profile = sdf_session_agg.groupBy(\"user_id\").agg(f.sum(\"sum(events)\"), f.sum(\"sum(views)\"), f.sum(\"sum(purchases)\"), f.sum(\"sum(carts)\"), f.sum(\"turnover\"), f.count(\"user_session\"), f.sum(\"successfull\"))\n",
    "\n",
    "sdf_customer_profile = sdf_customer_profile.withColumn(\"avg_turnover_per_session\", (sdf_customer_profile[\"sum(turnover)\"] / sdf_customer_profile[\"count(user_session)\"]))\n",
    "sdf_customer_profile = sdf_customer_profile.withColumn(\"avg_events_per_session\", (sdf_customer_profile[\"sum(sum(events))\"] / sdf_customer_profile[\"count(user_session)\"]))\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_customer_profile.show()\n",
    "\n",
    "#%% md\n",
    "\n",
    "### Correlation matrix\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Daytime - Correlation Matrix\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_time = sdf_raw.select(\"event_time\", \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\")\n",
    "sdf_corr_time = sdf_corr_time.withColumn(\"hour\", f.hour(\"event_time\"))\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_time = sdf_corr_time.withColumn('Morning', f.when((f.col('hour')>=6) & (f.col('hour')<12), f.lit(1)).otherwise(0))\n",
    "sdf_corr_time = sdf_corr_time.withColumn('Afternoon', f.when((f.col('hour')>=12) & (f.col('hour')<18), f.lit(1)).otherwise(0))\n",
    "sdf_corr_time = sdf_corr_time.withColumn('Evening', f.when(f.col('hour') > 18 , f.lit(1)).otherwise(0))\n",
    "sdf_corr_time = sdf_corr_time.withColumn('Night', f.when(f.col('hour') < 6, f.lit(1)).otherwise(0))\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_time = sdf_corr_time.select(\"Morning\", \"Afternoon\", \"Evening\", \"Night\", \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\")\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_time.toPandas().corr().style.background_gradient(cmap='bwr')\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Weekday - Correlation Matrix\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_dayofweek = sdf_raw.select(\"dayofweek\", \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\")\n",
    "\n",
    "#%%\n",
    "\n",
    "# One-hot-encoding\n",
    "sdf_corr_dayofweek = sdf_corr_dayofweek.withColumn('Sunday', f.when(f.col('dayofweek') == '1', f.lit(1)).otherwise(0))\n",
    "sdf_corr_dayofweek = sdf_corr_dayofweek.withColumn('Monday', f.when(f.col('dayofweek') == '2', f.lit(1)).otherwise(0))\n",
    "sdf_corr_dayofweek = sdf_corr_dayofweek.withColumn('Tuesday', f.when(f.col('dayofweek') == '3', f.lit(1)).otherwise(0))\n",
    "sdf_corr_dayofweek = sdf_corr_dayofweek.withColumn('Wednesday', f.when(f.col('dayofweek') == '4', f.lit(1)).otherwise(0))\n",
    "sdf_corr_dayofweek = sdf_corr_dayofweek.withColumn('Thursday', f.when(f.col('dayofweek') == '5', f.lit(1)).otherwise(0))\n",
    "sdf_corr_dayofweek = sdf_corr_dayofweek.withColumn('Friday', f.when(f.col('dayofweek') == '6', f.lit(1)).otherwise(0))\n",
    "sdf_corr_dayofweek = sdf_corr_dayofweek.withColumn('Saturday', f.when(f.col('dayofweek') == '7', f.lit(1)).otherwise(0))\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_dayofweek = sdf_corr_dayofweek.select(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\",  \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\" )\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_dayofweek.toPandas().corr().style.background_gradient(cmap='bwr')\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Category Class - Correlation Matrix\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_category_class = sdf_raw.select(\"category_class\", \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\")\n",
    "\n",
    "#%%\n",
    "\n",
    "# One-hot-encoding\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('computers', f.when(f.col('category_class') == 'computers', f.lit(1)).otherwise(0))\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('auto', f.when(f.col('category_class') == 'auto', f.lit(1)).otherwise(0))\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('apparel', f.when(f.col('category_class') == 'apparel', f.lit(1)).otherwise(0))\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('appliances', f.when(f.col('category_class') == 'appliances', f.lit(1)).otherwise(0))\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('furniture', f.when(f.col('category_class') == 'furniture', f.lit(1)).otherwise(0))\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('accessories', f.when(f.col('category_class') == 'accessories', f.lit(1)).otherwise(0))\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('electronics', f.when(f.col('category_class') == 'electronics', f.lit(1)).otherwise(0))\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('construction', f.when(f.col('category_class') == 'construction', f.lit(1)).otherwise(0))\n",
    "sdf_corr_category_class = sdf_corr_category_class.withColumn('not_defined', f.when(f.col('category_class') == 'not defined', f.lit(1)).otherwise(0))\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_category_class = sdf_corr_category_class.select(\"computers\", \"auto\", \"apparel\", \"appliances\", \"furniture\", \"accessories\", \"electronics\", \"construction\", \"not_defined\", \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\" )\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_category_class.toPandas().corr().style.background_gradient(cmap='bwr')\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Month - Correlation Matrix\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_month = sdf_raw.select(\"dayofmonth\", \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\")\n",
    "\n",
    "#%%\n",
    "\n",
    "# One-hot-encoding\n",
    "sdf_corr_month = sdf_corr_month.withColumn('Beginningofmonth', f.when(f.col('dayofmonth')<10, f.lit(1)).otherwise(0))\n",
    "sdf_corr_month = sdf_corr_month.withColumn('Middleofmonth', f.when((f.col('dayofmonth')>=10) & (f.col('dayofmonth')<20), f.lit(1)).otherwise(0))\n",
    "sdf_corr_month = sdf_corr_month.withColumn('Endofmonth', f.when(f.col('dayofmonth') > 20 , f.lit(1)).otherwise(0))\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_month = sdf_corr_month.select(\"Beginningofmonth\", \"Middleofmonth\", \"Endofmonth\", \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\" )\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_month.toPandas().corr().style.background_gradient(cmap='bwr')\n",
    "\n",
    "#%% md\n",
    "\n",
    "#### Price - Correlation Matrix\n",
    "\n",
    "#%%\n",
    "\n",
    "sdf_corr_price = sdf_raw.select(\"price\", \"turnover\", \"bougth_quantity\", \"viewed_quantity\", \"cart_quantity\")\n",
    "sdf_corr_price.toPandas().corr().style.background_gradient(cmap='bwr')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}